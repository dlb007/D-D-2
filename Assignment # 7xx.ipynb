{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Seven: Textual Analysis\n",
    "\n",
    "In this exercise, I took the subheadings supplied to help organize and format, code, and draw inspiration and support from the referred text to continue the documented notebook to bring in viable and researchable textual analysis toward the strength of a collection of written texts (aka multi-document corpus).\n",
    "\n",
    "In this weeks journey, the workflow explored the focuses below:\n",
    "\n",
    "- (Stage #1) Import at least three documents you would like to compare (from text files, or using another format for a challenge.)\n",
    "- (Stage #2) Preprocess the text and create a tokenized corpus from the text of the imported documents.\n",
    "- (Stage #3) Create a document term matrix to enable comparative textual analysis across the full set of documents\n",
    "- (Stage #4) Chart at least one comparison between the documents, using word frequency to map the text\n",
    "- (Stage #5) Calculate the Euclidean distance between the documents, using two key words as the point of comparison\n",
    "- As a (Bonus Challenge), consider trying one of the other types of distance modeling described in the text.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage One - Import at least three documents you would like to compare (from text files, or using another format for a challenge.)\n",
    "\n",
    "I went into the referenced resource (Project Gutenberg) presented in class and started to explore.  Using a search of just film, I was able to come across many inviting titles to use to fulfill a viable area of reference for enhancing and expanding upon my notebook, such as the \"Twelve Stories and a Dream\" of H.G. Wells to Night of the Living Dead of George A. Romero, both of which just from their titles granted immediate interest due to the upcoming holiday and to their established notoriety as film influencers. The code direction below allowed the resources to become organized alphabetically and logged into the project using shortened titles from the .txt files downloaded. To name the documents, I was drawn to using interest in unique words and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atomic.txt\n",
      "behind.txt\n",
      "boys.txt\n",
      "fear.txt\n",
      "first.txt\n",
      "french.txt\n",
      "island.txt\n",
      "life.txt\n",
      "living.txt\n",
      "london.txt\n",
      "motion.txt\n",
      "mystery.txt\n",
      "snowbound.txt\n",
      "togoland.txt\n",
      "truth.txt\n",
      "war.txt\n",
      "wells.txt\n",
      "behind.txt\n",
      "ï»¿The Project Gutenberg EBook of Behind the Screen, by Samuel Goldwyn\n",
      "\n",
      "This eBook is for the use of a\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.tokenize\n",
    "\n",
    "# download the most recent punkt package\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "import os\n",
    "documents = []\n",
    "titles =[]\n",
    "path = 'Films/'\n",
    "with os.scandir(path) as entries:\n",
    "    for entry in entries:\n",
    "        print(entry.name)\n",
    "        f = open(f'{path}\\{entry.name}',encoding='utf-8')\n",
    "        text = f.read()\n",
    "        documents.append(text)\n",
    "        titles.append(entry.name)\n",
    "print(titles[1])\n",
    "print(documents[1][0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Two - Preprocess the text and create a tokenized corpus from the text of the imported documents\n",
    "\n",
    "\n",
    "The process attained with this segmented coding helped remove repetitions of punctuation distractions by filtration of \"non-punctuation tokens\" (Karsdopp, Kestemont, Riddell 84).  Where implementing the function code of PUNCT_RE = re.compile(r'[^\\w\\s]+$'), enabled for identificational aspects geared toward seeing if an \"input string is either a single punctuation marker or a sequence\" (84). Accompanied with the use of [^\\w\\s]+$, it ensures that \"a string is only matched if it solely consists of punctuation characters\" (84).  Lastly, utilizing  is_punct(string) guided the process by identifying that non-punctuation tokens could be utilized toward success by incorporating a loop/list. Which interestingly ended with a print(tokenized[0][11]) as presented united."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "united\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "PUNCT_RE = re.compile(r'[^\\w\\s]+$')\n",
    "\n",
    "def is_punct(string):\n",
    "    \"\"\"Check if STRING is a punctuation marker or a sequence of\n",
    "       punctuation markers.\n",
    "    \"\"\"\n",
    "    return PUNCT_RE.match(string) is not None\n",
    "def preprocess_text(text, language, lowercase=True):\n",
    "    \"\"\"Preprocess a text.\n",
    "\n",
    "    Perform a text preprocessing procedure, which transforms a string\n",
    "    object into a list of word tokens without punctuation markers.\n",
    "\n",
    "    \"\"\"\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(text, language=language)\n",
    "    tokens = [token for token in tokens if not is_punct(token)]\n",
    "    return tokens\n",
    "tokenized = []\n",
    "for text in documents:\n",
    "    tokenized.append(preprocess_text(text, \"english\"))\n",
    "\n",
    "print(tokenized[0][11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Three: Create a document term matrix to enable comparative textual analysis across the full set of documents\n",
    "\n",
    "dictionary for the whole set.\n",
    "\n",
    "Bringing in a document term matrix enabled a viable mode of textual analysis that would represent a reason and expansion through the entire listed and introduced data attained to have an opportunity to be compared correctly and openly, as a whole set.\n",
    "\n",
    "Enveloping the format to use the function code of extract_vocabulary() enables a concise way to extract components included in the vocabulary of \"a tokenized corpus given a minimum and a  maximum frequency count\" (Karsdopp, Kestemont, Riddell 87). This coded use helps the amount and size be put through a filter to reduce using fiding and location thresholds and allowances of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'_i_\", \"'ad\", \"'ail\", \"'and\", \"'ands\", \"'ardly\", \"'arf\", \"'arf-crowns\", \"'as-is\", \"'at\", \"'ave\", \"'black\", \"'blighty\", \"'blow\", \"'boo\", \"'bosche\", \"'bus\", \"'but\", \"'certainly\", \"'come\", \"'d\", \"'dead\", \"'eard\", \"'eart\", \"'em\", \"'er\", \"'ere\", \"'fan\", \"'flying\", \"'go\", \"'here\", \"'hush\", \"'if\", \"'im\", \"'ims\", \"'ired\", \"'is\", \"'it\", \"'jacob\", \"'lanwick\", \"'lark\", \"'life\", \"'ll\", \"'m\", \"'macaroni\", \"'mar\", \"'miss\", \"'movie\", \"'movie-man\", \"'my\", \"'no\", \"'not\", \"'ole\", \"'olidays\", \"'ome\", \"'orse\", \"'ouse\", \"'ow\", \"'phone\", \"'poetry\", \"'re\", \"'s\", \"'seeing\", \"'strafe\", \"'strafing\", \"'that\", \"'the\", \"'there\", \"'they\", \"'thou\", \"'treasure\", \"'uff\", \"'undreds\", \"'urt\", \"'varsity\", \"'ve\", \"'well\", \"'what\", \"'white\", \"'why\", \"'yes\", \"'you\", '.zip', '//gallica.bnf.fr', '//gutenberg.org/license', '//pglaf.org', '//pglaf.org/donate', '//pglaf.org/fundraising', '//www.gutenberg.org', '//www.gutenberg.org/about/contact', '//www.gutenberg.org/fundraising/donate', '//www.gutenberg.org/fundraising/pglaf', '//www.gutenberg.org/license', '//www.nv.doe.gov/news', '//www.osti.gov/historicalfilms/filmlist.html', '//www.pgdp.net', '//www.pglaf.org', '/d/', '/z/', '/Å¡/']\n"
     ]
    }
   ],
   "source": [
    "def extract_vocabulary(tokenized_corpus, min_count=1, max_count=float('inf')):\n",
    "    \"\"\"Extract a vocabulary from a tokenized corpus.\n",
    "\n",
    "    Arguments:\n",
    "        tokenized_corpus (list): a tokenized corpus represented, list\n",
    "            of lists of strings.\n",
    "        min_count (int, optional): the minimum occurrence count of a\n",
    "            vocabulary item in the corpus.\n",
    "        max_count (int, optional): the maximum occurrence count of a\n",
    "            vocabulary item in the corpus. Defaults to inf.\n",
    "\n",
    "    Returns:\n",
    "        list: An alphabetically ordered list of unique words in the\n",
    "            corpus, of which the frequencies adhere to the specified\n",
    "            minimum and maximum count.\n",
    "\n",
    "    Examples:\n",
    "        >>> corpus = [['the', 'man', 'love', 'man', 'the'],\n",
    "                      ['the', 'love', 'book', 'wise', 'drama'],\n",
    "                      ['a', 'story', 'book', 'drama']]\n",
    "        >>> extract_vocabulary(corpus, min_count=2)\n",
    "        ['book', 'drama', 'love', 'man', 'the']\n",
    "\n",
    "    \"\"\"\n",
    "    vocabulary = collections.Counter()\n",
    "    for document in tokenized_corpus:\n",
    "        vocabulary.update(document)\n",
    "    vocabulary = {word for word, count in vocabulary.items()\n",
    "                  if count >= min_count and count <= max_count}\n",
    "    return sorted(vocabulary)\n",
    "import collections\n",
    "vocabulary = extract_vocabulary(tokenized, min_count=2)\n",
    "print(vocabulary[0:100])\n",
    "\n",
    "def corpus2dtm(tokenized_corpus, vocabulary):\n",
    "    \"\"\"Transform a tokenized corpus into a document-term matrix.\n",
    "\n",
    "    Arguments:\n",
    "        tokenized_corpus (list): a tokenized corpus as a list of\n",
    "        lists of strings. vocabulary (list): An list of unique words.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lists representing the frequency of each term\n",
    "              in `vocabulary` for each document in the corpus.\n",
    "\n",
    "    Examples:\n",
    "        >>> tokenized_corpus = [['the', 'man', 'man', 'smart'],\n",
    "                                ['a', 'the', 'man' 'love'],\n",
    "                                ['love', 'book', 'journey']]\n",
    "        >>> vocab = ['book', 'journey', 'man', 'love']\n",
    "        >>> corpus2dtm(tokenized_corpus, vocabulary)\n",
    "        [[0, 0, 2, 0], [0, 0, 1, 1], [1, 1, 0, 1]]\n",
    "\n",
    "    \"\"\"\n",
    "    document_term_matrix = []\n",
    "    for document in tokenized_corpus:\n",
    "        document_counts = collections.Counter(document)\n",
    "        row = [document_counts[word] for word in vocabulary]\n",
    "        document_term_matrix.append(row)\n",
    "    return document_term_matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "document_term_matrix = np.array(corpus2dtm(tokenized, vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Four: Chart at least one comparison between the documents, using word frequency to map the text\n",
    "\n",
    "Revealing a comparison accompanied by word frequencies to help map the text, Python's use of data structure \"only records distinct elements,\" which brings on the identification of \"all words appearing in it are unique.\" Similarly,  we could construct a vocabulary which excludes the n most\" (Karsdopp, Kestemont, Riddell 87). And since use was enabled within three genres, with the code formation of tr_means = document_term_matrix[genres == 'they'].mean(axis=0), the journey became visual in a sense (96). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She: [  0 604  28 861   0   0   0   0   0  75  20 357 186 102  24  19 314]\n",
      "He: [   2  602  569  838    2    2    2   71    2  365    7 1171  574  629\n",
      "   45  536 1568]\n",
      "They: [  3 116 562 177   3   3   3  18   3 487  75 157 292 454  23 461 292]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "girl_id = vocabulary.index('she')\n",
    "boy_id = vocabulary.index('he')\n",
    "they_id = vocabulary.index('they')\n",
    "\n",
    "girl_counts = document_term_matrix[:, girl_id]\n",
    "boy_counts = document_term_matrix[:, boy_id]\n",
    "they_counts = document_term_matrix[:, they_id]\n",
    "print(\"She: \" + str(girl_counts))\n",
    "print(\"He: \" + str(boy_counts))\n",
    "print(\"They: \" + str(they_counts))\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(len(titles))\n",
    "width = 1/(len(titles))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width, girl_counts, width, label='She')\n",
    "rects2 = ax.bar(x, boy_counts, width, label='He')\n",
    "rects3 = ax.bar(x + width, they_counts, width, label='They')\n",
    "\n",
    "\n",
    "ax.set_ylabel('Word Count')\n",
    "ax.set_title('Pronoun Frequency')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(titles)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Five: Calculate the Euclidean distance between the documents, using two key words as the point of comparison\n",
    "\n",
    "Note: Nested four-loop compares and helps to avoid redundancy (loop logic solutions).\n",
    "\n",
    "Euclidean distance's use allows for \"calculating the distance between two documents in a space, ... looks at the exact coordinates of the two documents: it connects them with a straight line, ...and returns the length of that line\" (Karsdopp, Kestemont, Riddell 100). As a result, the comparisons or calculated distances between supplied .txt brought interest and allowed comparisons to take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atomic.txt vs behind.txt: 610.5481144021329\n",
      "atomic.txt vs boys.txt: 796.2223307594431\n",
      "atomic.txt vs fear.txt: 853.9156867044895\n",
      "atomic.txt vs first.txt: 0.0\n",
      "atomic.txt vs french.txt: 0.0\n",
      "atomic.txt vs island.txt: 0.0\n",
      "atomic.txt vs life.txt: 70.61161377563892\n",
      "atomic.txt vs living.txt: 0.0\n",
      "atomic.txt vs london.txt: 605.0\n",
      "atomic.txt vs motion.txt: 72.17340230306452\n",
      "atomic.txt vs mystery.txt: 1179.1000805699234\n",
      "atomic.txt vs snowbound.txt: 640.862699803944\n",
      "atomic.txt vs togoland.txt: 772.3535459878461\n",
      "atomic.txt vs truth.txt: 47.4236228055175\n",
      "atomic.txt vs war.txt: 703.5055081518552\n",
      "atomic.txt vs wells.txt: 1592.4437195706478\n",
      "behind.txt vs boys.txt: 447.21918563496354\n",
      "behind.txt vs fear.txt: 243.75602556654883\n",
      "behind.txt vs first.txt: 610.5481144021329\n",
      "behind.txt vs french.txt: 610.5481144021329\n",
      "behind.txt vs island.txt: 610.5481144021329\n",
      "behind.txt vs life.txt: 539.9675916200897\n",
      "behind.txt vs living.txt: 610.5481144021329\n",
      "behind.txt vs london.txt: 440.23857168585306\n",
      "behind.txt vs motion.txt: 596.4109321600334\n",
      "behind.txt vs mystery.txt: 570.4752404793744\n",
      "behind.txt vs snowbound.txt: 178.2133552795637\n",
      "behind.txt vs togoland.txt: 339.0766874911928\n",
      "behind.txt vs truth.txt: 564.7105453238854\n",
      "behind.txt vs war.txt: 351.25631666918105\n",
      "behind.txt vs wells.txt: 981.902235459315\n",
      "boys.txt vs fear.txt: 469.6658386555275\n",
      "boys.txt vs first.txt: 796.2223307594431\n",
      "boys.txt vs french.txt: 796.2223307594431\n",
      "boys.txt vs island.txt: 796.2223307594431\n",
      "boys.txt vs life.txt: 737.5228810009897\n",
      "boys.txt vs living.txt: 796.2223307594431\n",
      "boys.txt vs london.txt: 217.34994824015948\n",
      "boys.txt vs motion.txt: 743.6484384438658\n",
      "boys.txt vs mystery.txt: 725.5542708853694\n",
      "boys.txt vs snowbound.txt: 270.04629232781554\n",
      "boys.txt vs togoland.txt: 123.54756169184401\n",
      "boys.txt vs truth.txt: 751.7293395897223\n",
      "boys.txt vs war.txt: 106.25441167311595\n",
      "boys.txt vs wells.txt: 1034.8434664237873\n",
      "fear.txt vs first.txt: 853.9156867044895\n",
      "fear.txt vs french.txt: 853.9156867044895\n",
      "fear.txt vs island.txt: 853.9156867044895\n",
      "fear.txt vs life.txt: 783.3070917590368\n",
      "fear.txt vs living.txt: 853.9156867044895\n",
      "fear.txt vs london.txt: 565.5342606774589\n",
      "fear.txt vs motion.txt: 837.2365257201814\n",
      "fear.txt vs mystery.txt: 333.600059952033\n",
      "fear.txt vs snowbound.txt: 287.9600666759195\n",
      "fear.txt vs togoland.txt: 347.0014409191985\n",
      "fear.txt vs truth.txt: 807.8149540581679\n",
      "fear.txt vs war.txt: 414.56000771902734\n",
      "fear.txt vs wells.txt: 739.0027063549903\n",
      "first.txt vs french.txt: 0.0\n",
      "first.txt vs island.txt: 0.0\n",
      "first.txt vs life.txt: 70.61161377563892\n",
      "first.txt vs living.txt: 0.0\n",
      "first.txt vs london.txt: 605.0\n",
      "first.txt vs motion.txt: 72.17340230306452\n",
      "first.txt vs mystery.txt: 1179.1000805699234\n",
      "first.txt vs snowbound.txt: 640.862699803944\n",
      "first.txt vs togoland.txt: 772.3535459878461\n",
      "first.txt vs truth.txt: 47.4236228055175\n",
      "first.txt vs war.txt: 703.5055081518552\n",
      "first.txt vs wells.txt: 1592.4437195706478\n",
      "french.txt vs island.txt: 0.0\n",
      "french.txt vs life.txt: 70.61161377563892\n",
      "french.txt vs living.txt: 0.0\n",
      "french.txt vs london.txt: 605.0\n",
      "french.txt vs motion.txt: 72.17340230306452\n",
      "french.txt vs mystery.txt: 1179.1000805699234\n",
      "french.txt vs snowbound.txt: 640.862699803944\n",
      "french.txt vs togoland.txt: 772.3535459878461\n",
      "french.txt vs truth.txt: 47.4236228055175\n",
      "french.txt vs war.txt: 703.5055081518552\n",
      "french.txt vs wells.txt: 1592.4437195706478\n",
      "island.txt vs life.txt: 70.61161377563892\n",
      "island.txt vs living.txt: 0.0\n",
      "island.txt vs london.txt: 605.0\n",
      "island.txt vs motion.txt: 72.17340230306452\n",
      "island.txt vs mystery.txt: 1179.1000805699234\n",
      "island.txt vs snowbound.txt: 640.862699803944\n",
      "island.txt vs togoland.txt: 772.3535459878461\n",
      "island.txt vs truth.txt: 47.4236228055175\n",
      "island.txt vs war.txt: 703.5055081518552\n",
      "island.txt vs wells.txt: 1592.4437195706478\n",
      "life.txt vs living.txt: 70.61161377563892\n",
      "life.txt vs london.txt: 553.531390257138\n",
      "life.txt vs motion.txt: 85.70297544426332\n",
      "life.txt vs mystery.txt: 1108.7474915416944\n",
      "life.txt vs snowbound.txt: 572.7870459429054\n",
      "life.txt vs togoland.txt: 708.1384045509748\n",
      "life.txt vs truth.txt: 26.476404589747453\n",
      "life.txt vs war.txt: 642.2413876417495\n",
      "life.txt vs wells.txt: 1521.8689168256246\n",
      "living.txt vs london.txt: 605.0\n",
      "living.txt vs motion.txt: 72.17340230306452\n",
      "living.txt vs mystery.txt: 1179.1000805699234\n",
      "living.txt vs snowbound.txt: 640.862699803944\n",
      "living.txt vs togoland.txt: 772.3535459878461\n",
      "living.txt vs truth.txt: 47.4236228055175\n",
      "living.txt vs war.txt: 703.5055081518552\n",
      "living.txt vs wells.txt: 1592.4437195706478\n",
      "london.txt vs motion.txt: 545.8094905734051\n",
      "london.txt vs mystery.txt: 870.9397223688904\n",
      "london.txt vs snowbound.txt: 285.84261403786525\n",
      "london.txt vs togoland.txt: 266.05450569385215\n",
      "london.txt vs truth.txt: 563.6452785218731\n",
      "london.txt vs war.txt: 172.96531444194238\n",
      "london.txt vs wells.txt: 1218.70176827639\n",
      "motion.txt vs mystery.txt: 1166.8847415233433\n",
      "motion.txt vs snowbound.txt: 607.1062509973028\n",
      "motion.txt vs togoland.txt: 728.3714711601492\n",
      "motion.txt vs truth.txt: 64.40496875241847\n",
      "motion.txt vs war.txt: 654.8564728243892\n",
      "motion.txt vs wells.txt: 1576.010786765116\n",
      "mystery.txt vs snowbound.txt: 612.0735249951593\n",
      "mystery.txt vs togoland.txt: 618.0396427414669\n",
      "mystery.txt vs truth.txt: 1133.9453249605997\n",
      "mystery.txt vs war.txt: 704.0177554579145\n",
      "mystery.txt vs wells.txt: 419.32564910818417\n",
      "snowbound.txt vs togoland.txt: 171.0818517552344\n",
      "snowbound.txt vs truth.txt: 593.4660900169445\n",
      "snowbound.txt vs war.txt: 173.21951391226105\n",
      "snowbound.txt vs wells.txt: 994.0\n",
      "togoland.txt vs truth.txt: 725.8216034260761\n",
      "togoland.txt vs war.txt: 93.26306878931231\n",
      "togoland.txt vs wells.txt: 952.87197461149\n",
      "truth.txt vs war.txt: 657.9703640742491\n",
      "truth.txt vs wells.txt: 1546.5736322593891\n",
      "war.txt vs wells.txt: 1045.7461451040592\n"
     ]
    }
   ],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    \"\"\"Compute the Euclidean distance between two vectors.\n",
    "\n",
    "    Note: ``numpy.linalg.norm(a - b)`` performs the\n",
    "    same calculation using a slightly faster method.\n",
    "\n",
    "    Arguments:\n",
    "        a (numpy.ndarray): a vector of floats or ints.\n",
    "        b (numpy.ndarray): a vector of floats or ints.\n",
    "\n",
    "    Returns:\n",
    "        float: The euclidean distance between vector a and b.\n",
    "\n",
    "    Examples:\n",
    "        >>> import numpy as np\n",
    "        >>> a = np.array([1, 4, 2, 8])\n",
    "        >>> b = np.array([2, 1, 4, 7])\n",
    "        >>> round(euclidean_distance(a, b), 2)\n",
    "        3.87\n",
    "\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "vectors = []\n",
    "for x in range(0, len(titles)):\n",
    "    vectors.append([they_counts[x],boy_counts[x]])\n",
    "\n",
    "for x in range(0, len(vectors)):\n",
    "    for y in range(x+1, len(vectors)):\n",
    "        comparison = euclidean_distance(np.array(vectors[x]),np.array(vectors[y]))\n",
    "        print(f'{titles[x]} vs {titles[y]}: {comparison}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre - Bonus Stage:\n",
    "\n",
    "Computation of a vector was introduced to enable more value within the Bonus Stage section to help choices of distant modeling, \"def vector_len(v):\" (Karsdopp, Kestemont, Riddell 99)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_len(v):\n",
    "    \"\"\"Compute the length (or norm) of a vector.\"\"\"\n",
    "    return np.sqrt(np.sum(v ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Stage: As a bonus challenge, consider trying one of the other types of distance modeling described in the text.\n",
    "\n",
    "Cosine distance \"is the well-known cosine  distance from geometry\" and differs from Euclidean distance beyond two points importance and bring forth the impact of their identities in arrows/vectors \"that find their offset in the space's origin\" (Karsdopp, Kestemont, Riddell 100). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Distance (#1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boy - they:       0.26\n",
      "boy - girl: 0.25\n",
      "they - girl: 0.59\n"
     ]
    }
   ],
   "source": [
    "def cosine_distance(a, b):\n",
    "    \"\"\"Compute the cosine distance between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        a (numpy.ndarray): a vector of floats or ints.\n",
    "        b (numpy.ndarray): a vector of floats or ints.\n",
    "\n",
    "    Returns:\n",
    "        float: cosine distance between vector a and b.\n",
    "\n",
    "    Note:\n",
    "        See also scipy.spatial.distance.cdist\n",
    "\n",
    "    Examples:\n",
    "        >>> import numpy as np\n",
    "        >>> a = np.array([1, 4, 2, 8])\n",
    "        >>> b = np.array([2, 1, 4, 7])\n",
    "        >>> round(cosine_distance(a, b), 2)\n",
    "        0.09\n",
    "\n",
    "    \"\"\"\n",
    "    return 1 - np.dot(a, b) / (vector_len(a) * vector_len(b))\n",
    "\n",
    "\n",
    "# %%\n",
    "tc = cosine_distance(boy_counts, they_counts)\n",
    "print(f'boy - they:       {tc:.2f}')\n",
    "\n",
    "ttc = cosine_distance(boy_counts, girl_counts)\n",
    "print(f'boy - girl: {ttc:.2f}')\n",
    "\n",
    "ctc = cosine_distance(they_counts, girl_counts)\n",
    "print(f'they - girl: {ctc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Distance (#2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boy - they:       0.26\n",
      "boy - girl: 0.25\n",
      "they - girl: 0.59\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def cosine_distance(a, b):\n",
    "    \"\"\"Compute the cosine distance between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        a (numpy.ndarray): a vector of floats or ints.\n",
    "        b (numpy.ndarray): a vector of floats or ints.\n",
    "\n",
    "    Returns:\n",
    "        float: cosine distance between vector a and b.\n",
    "\n",
    "    Note:\n",
    "        See also scipy.spatial.distance.cdist\n",
    "\n",
    "    Examples:\n",
    "        >>> import numpy as np\n",
    "        >>> a = np.array([1, 4, 2, 8])\n",
    "        >>> b = np.array([2, 1, 4, 7])\n",
    "        >>> round(cosine_distance(a, b), 2)\n",
    "        0.09\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1 - np.dot(a, b) / (vector_len(a) * vector_len(b))\n",
    "\n",
    "\n",
    "# %%\n",
    "tc = cosine_distance(boy_counts, they_counts)\n",
    "print(f'boy - they:       {tc:.2f}')\n",
    "\n",
    "ttc = cosine_distance(boy_counts, girl_counts)\n",
    "print(f'boy - girl: {ttc:.2f}')\n",
    "\n",
    "ctc = cosine_distance(they_counts, girl_counts)\n",
    "print(f'they - girl: {ctc:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City Block Distance\n",
    "\n",
    "City block distance (aka Manhattan distance/L1 distance) \"is a metric which computes the distance between two  points in space as the sum of the absolute differences of their coordinates in  space\" (Karsdopp, Kestemont, Riddell 102). In this code example, I \"plotted the individual paths between the data  points\" (104). and even though it is not used that frequently in text analysis, it \"is a well-known distance function whose \"inner workings\" are not too hard to understand\" (105). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_block_distance(a, b):\n",
    "    \"\"\"Compute the city block distance between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        a (numpy.ndarray): a vector of floats or ints.\n",
    "        b (numpy.ndarray): a vector of floats or ints.\n",
    "\n",
    "    Returns:\n",
    "        {int, float}: The city block distance between vector a and b.\n",
    "\n",
    "    Examples:\n",
    "        >>> import numpy as np\n",
    "        >>> a = np.array([1, 4, 2, 8])\n",
    "        >>> b = np.array([2, 1, 4, 7])\n",
    "        >>> city_block_distance(a, b)\n",
    "        7\n",
    "\n",
    "    \"\"\"\n",
    "    return np.abs(a - b).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = euclidean_distance(boy_counts, they_counts)  \n",
    "print(f'they - boy: {tc:.2f}')  \n",
    "\n",
    "ttc = euclidean_distance(they_counts, girl_counts)  \n",
    "print(f'they - girl: {ttc:.2f}') \n",
    "\n",
    "ctc = euclidean_distance(boy_counts, girl_counts) \n",
    "print(f' boy - girl: {ctc:.2f}') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Genre properties can moreover be extracted at various levels from texts, including style, themes, and settings, and successful authors often like to blend genres  (e.g., a âhistorical thrillerâ). Genre classifications therefore rarely go uncontested and their application can be a highly subjective matter, where personal  taste or the paradigm a scholar works in will play a significant role. Because of  the (inter)subjectivity that is involved in genre studies, quantitative approaches  can offer a valuable second opinion on genetic classifications, like the one  offered by Paul FiÃ¨vre. Are there any lexical differences between the texts in  this corpus that would seem to correlate, or perhaps contradict, the classification proposed? Can the textual properties in a bag-of-words model shed new  light on the special status of the tragi-comÃ©dies? And so on.\" (p. 93). Princeton University Press. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "598d64e08a06bfba065a99698ec5bbec753236817de80e4f3bcf221574aa140c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
