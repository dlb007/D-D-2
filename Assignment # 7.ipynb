{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise Seven: Textual Analysis\r\n",
    "\r\n",
    "In this exercise, I will not be providing the subheadings. Work from our code examples and the textbooks to construct a well-documented notebook that provides a model for initial textual analysis of a multi-document corpus.\r\n",
    "\r\n",
    "Your workflow should:\r\n",
    "\r\n",
    "(This notebook provides a model for initial textual analysis of a multi-document corpus.\r\n",
    "\r\n",
    "Your workflow should:\r\n",
    "\r\n",
    "- Import at least three documents you would like to compare (from text files, or using another format for a challenge.)\r\n",
    "- Preprocess the text and create a tokenized corpus from the text of the imported documents.\r\n",
    "- Create a document term matrix to enable comparative textual analysis across the full set of documents\r\n",
    "- Chart at least one comparison between the documents, using word frequency to map the text\r\n",
    "- Calculate the Euclidean distance between the documents, using two key words as the point of comparison\r\n",
    "- As a bonus challenge, consider trying one of the other types of distance modeling described in the text.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stage One - Import at least three documents you would like to compare (from text files, or using another format for a challenge.)\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import nltk\r\n",
    "import nltk.tokenize\r\n",
    "\r\n",
    "# download the most recent punkt package\r\n",
    "nltk.download('punkt', quiet=True)\r\n",
    "\r\n",
    "import os\r\n",
    "documents = []\r\n",
    "titles =[]\r\n",
    "path = 'texts/'\r\n",
    "with os.scandir(path) as entries:\r\n",
    "    for entry in entries:\r\n",
    "        print(entry.name)\r\n",
    "        f = open(f'{path}\\{entry.name}',encoding='utf-8')\r\n",
    "        text = f.read()\r\n",
    "        documents.append(text)\r\n",
    "        titles.append(entry.name)\r\n",
    "print(titles[1])\r\n",
    "print(documents[1][0:100])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stage Two - Preprocess the text and create a tokenized corpus from the text of the imported documents"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\r\n",
    "PUNCT_RE = re.compile(r'[^\\w\\s]+$')\r\n",
    "\r\n",
    "def is_punct(string):\r\n",
    "    \"\"\"Check if STRING is a punctuation marker or a sequence of\r\n",
    "       punctuation markers.\r\n",
    "    \"\"\"\r\n",
    "    return PUNCT_RE.match(string) is not None\r\n",
    "def preprocess_text(text, language, lowercase=True):\r\n",
    "    \"\"\"Preprocess a text.\r\n",
    "\r\n",
    "    Perform a text preprocessing procedure, which transforms a string\r\n",
    "    object into a list of word tokens without punctuation markers.\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    if lowercase:\r\n",
    "        text = text.lower()\r\n",
    "    tokens = nltk.tokenize.word_tokenize(text, language=language)\r\n",
    "    tokens = [token for token in tokens if not is_punct(token)]\r\n",
    "    return tokens\r\n",
    "tokenized = []\r\n",
    "for text in documents:\r\n",
    "    tokenized.append(preprocess_text(text, \"english\"))\r\n",
    "\r\n",
    "print(tokenized[0][11])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stage Three: Create a document term matrix to enable comparative textual analysis across the full set of documents"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_vocabulary(tokenized_corpus, min_count=1, max_count=float('inf')):\r\n",
    "    \"\"\"Extract a vocabulary from a tokenized corpus.\r\n",
    "\r\n",
    "    Arguments:\r\n",
    "        tokenized_corpus (list): a tokenized corpus represented, list\r\n",
    "            of lists of strings.\r\n",
    "        min_count (int, optional): the minimum occurrence count of a\r\n",
    "            vocabulary item in the corpus.\r\n",
    "        max_count (int, optional): the maximum occurrence count of a\r\n",
    "            vocabulary item in the corpus. Defaults to inf.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        list: An alphabetically ordered list of unique words in the\r\n",
    "            corpus, of which the frequencies adhere to the specified\r\n",
    "            minimum and maximum count.\r\n",
    "\r\n",
    "    Examples:\r\n",
    "        >>> corpus = [['the', 'man', 'love', 'man', 'the'],\r\n",
    "                      ['the', 'love', 'book', 'wise', 'drama'],\r\n",
    "                      ['a', 'story', 'book', 'drama']]\r\n",
    "        >>> extract_vocabulary(corpus, min_count=2)\r\n",
    "        ['book', 'drama', 'love', 'man', 'the']\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    vocabulary = collections.Counter()\r\n",
    "    for document in tokenized_corpus:\r\n",
    "        vocabulary.update(document)\r\n",
    "    vocabulary = {word for word, count in vocabulary.items()\r\n",
    "                  if count >= min_count and count <= max_count}\r\n",
    "    return sorted(vocabulary)\r\n",
    "import collections\r\n",
    "vocabulary = extract_vocabulary(tokenized, min_count=2)\r\n",
    "print(vocabulary[0:100])\r\n",
    "\r\n",
    "def corpus2dtm(tokenized_corpus, vocabulary):\r\n",
    "    \"\"\"Transform a tokenized corpus into a document-term matrix.\r\n",
    "\r\n",
    "    Arguments:\r\n",
    "        tokenized_corpus (list): a tokenized corpus as a list of\r\n",
    "        lists of strings. vocabulary (list): An list of unique words.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        list: A list of lists representing the frequency of each term\r\n",
    "              in `vocabulary` for each document in the corpus.\r\n",
    "\r\n",
    "    Examples:\r\n",
    "        >>> tokenized_corpus = [['the', 'man', 'man', 'smart'],\r\n",
    "                                ['a', 'the', 'man' 'love'],\r\n",
    "                                ['love', 'book', 'journey']]\r\n",
    "        >>> vocab = ['book', 'journey', 'man', 'love']\r\n",
    "        >>> corpus2dtm(tokenized_corpus, vocabulary)\r\n",
    "        [[0, 0, 2, 0], [0, 0, 1, 1], [1, 1, 0, 1]]\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    document_term_matrix = []\r\n",
    "    for document in tokenized_corpus:\r\n",
    "        document_counts = collections.Counter(document)\r\n",
    "        row = [document_counts[word] for word in vocabulary]\r\n",
    "        document_term_matrix.append(row)\r\n",
    "    return document_term_matrix\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "document_term_matrix = np.array(corpus2dtm(tokenized, vocabulary))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stage Four: Chart at least one comparison between the documents, using word frequency to map the text"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "girl_id = vocabulary.index('she')\r\n",
    "boy_id = vocabulary.index('he')\r\n",
    "they_id = vocabulary.index('they')\r\n",
    "\r\n",
    "girl_counts = document_term_matrix[:, girl_id]\r\n",
    "boy_counts = document_term_matrix[:, boy_id]\r\n",
    "they_counts = document_term_matrix[:, they_id]\r\n",
    "print(\"She: \" + str(girl_counts))\r\n",
    "print(\"He: \" + str(boy_counts))\r\n",
    "print(\"They: \" + str(they_counts))\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "x = np.arange(len(titles))\r\n",
    "width = 1/(len(titles))\r\n",
    "\r\n",
    "fig, ax = plt.subplots()\r\n",
    "rects1 = ax.bar(x - width, girl_counts, width, label='She')\r\n",
    "rects2 = ax.bar(x, boy_counts, width, label='He')\r\n",
    "rects3 = ax.bar(x + width, they_counts, width, label='They')\r\n",
    "\r\n",
    "\r\n",
    "ax.set_ylabel('Word Count')\r\n",
    "ax.set_title('Pronoun Frequency')\r\n",
    "ax.set_xticks(x)\r\n",
    "ax.set_xticklabels(titles)\r\n",
    "ax.legend()\r\n",
    "\r\n",
    "fig.tight_layout()\r\n",
    "\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stage Five: Calculate the Euclidean distance between the documents, using two key words as the point of comparison"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def euclidean_distance(a, b):\r\n",
    "    \"\"\"Compute the Euclidean distance between two vectors.\r\n",
    "\r\n",
    "    Note: ``numpy.linalg.norm(a - b)`` performs the\r\n",
    "    same calculation using a slightly faster method.\r\n",
    "\r\n",
    "    Arguments:\r\n",
    "        a (numpy.ndarray): a vector of floats or ints.\r\n",
    "        b (numpy.ndarray): a vector of floats or ints.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        float: The euclidean distance between vector a and b.\r\n",
    "\r\n",
    "    Examples:\r\n",
    "        >>> import numpy as np\r\n",
    "        >>> a = np.array([1, 4, 2, 8])\r\n",
    "        >>> b = np.array([2, 1, 4, 7])\r\n",
    "        >>> round(euclidean_distance(a, b), 2)\r\n",
    "        3.87\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\r\n",
    "vectors = []\r\n",
    "for x in range(0, len(titles)):\r\n",
    "    vectors.append([they_counts[x],boy_counts[x]])\r\n",
    "\r\n",
    "for x in range(0, len(vectors)):\r\n",
    "    for y in range(x+1, len(vectors)):\r\n",
    "        comparison = euclidean_distance(np.array(vectors[x]),np.array(vectors[y]))\r\n",
    "        print(f'{titles[x]} vs {titles[y]}: {comparison}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bonus Stage:\r\n",
    "\r\n",
    "As a bonus challenge, consider trying one of the other types of distance modeling described in the text"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "598d64e08a06bfba065a99698ec5bbec753236817de80e4f3bcf221574aa140c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}