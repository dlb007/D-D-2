{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Seven: Textual Analysis\n",
    "\n",
    "In this exercise, I took the subheadings supplied to help organize and format, code, and draw inspiration and support from the referred text to continue the documented notebook to bring in viable and researchable textual analysis toward the strength of a collection of written texts (aka multi-document corpus).\n",
    "\n",
    "In this weeks journey, the workflow explored the focuses below:\n",
    "\n",
    "- (Stage #1) Import at least three documents you would like to compare (from text files, or using another format for a challenge.)\n",
    "- (Stage #2) Preprocess the text and create a tokenized corpus from the text of the imported documents.\n",
    "- (Stage #3) Create a document term matrix to enable comparative textual analysis across the full set of documents\n",
    "- (Stage #4) Chart at least one comparison between the documents, using word frequency to map the text\n",
    "- (Stage #5) Calculate the Euclidean distance between the documents, using two key words as the point of comparison\n",
    "- As a (Bonus Challenge), consider trying one of the other types of distance modeling described in the text.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage One - Import at least three documents you would like to compare (from text files, or using another format for a challenge.)\n",
    "\n",
    "I went into the referenced resource (Project Gutenberg) presented in class and started to explore.  Using a search of just film, I was able to come across many inviting titles to use to fulfill a viable area of reference for enhancing and expanding upon my notebook, such as the \"Twelve Stories and a Dream\" of H.G. Wells to Night of the Living Dead of George A. Romero, both of which just from their titles granted immediate interest due to the upcoming holiday and to their established notoriety as film influencers. The code direction below allowed the resources to become organized alphabetically and logged into the project using shortened titles from the .txt files downloaded. To name the documents, I was drawn to using interest in unique words and text. And with the use of OS or observational conversation tool, I decided to streamline down to just seven titles from 16 to help the visualizer within this portal become more presentable and streamlined by navigating between them toward the interest of film topics and subjects found within the folder housing the collection of .txt files.\n",
    "\n",
    "- Behind the Screen (behind.txt)\n",
    "- The Moving Picture Boys on the War Front (boys.txt)\n",
    "- The Film of Fear (fear.txt)\n",
    "- Night of the Living Dead (living.txt)\n",
    "- The Film Mystery (mystery.txt)\n",
    "- A Camera Actress in the Wilds of Togoland (togoland.txt)\n",
    "- Twelve Stories and a Dream (wells.txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behind.txt\n",
      "boys.txt\n",
      "fear.txt\n",
      "living.txt\n",
      "mystery.txt\n",
      "togoland.txt\n",
      "wells.txt\n",
      "boys.txt\n",
      "ï»¿The Project Gutenberg EBook of The Moving Picture Boys on the War Front, by \n",
      "Victor Appleton\n",
      "\n",
      "This \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.tokenize\n",
    "\n",
    "# download the most recent punkt package\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "import os\n",
    "documents = []\n",
    "titles =[]\n",
    "path = 'Films/'\n",
    "with os.scandir(path) as entries:\n",
    "    for entry in entries:\n",
    "        print(entry.name)\n",
    "        f = open(f'{path}\\{entry.name}',encoding='utf-8')\n",
    "        text = f.read()\n",
    "        documents.append(text)\n",
    "        titles.append(entry.name)\n",
    "print(titles[1])\n",
    "print(documents[1][0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Two - Preprocess the text and create a tokenized corpus from the text of the imported documents\n",
    "\n",
    "\n",
    "The process attained with this segmented coding helped remove repetitions of punctuation distractions by filtration of \"non-punctuation tokens\" (Karsdopp, Kestemont, Riddell 84).  Where implementing the function code of PUNCT_RE = re.compile(), enabled for identificational aspects geared toward seeing if an \"input string is either a single punctuation marker or a sequence\" (84). Accompanied with the use of [^\\w\\s]+$, it ensures that \"a string is only matched if it solely consists of punctuation characters\" (84).  Lastly, utilizing  is_punct(string) guided the process by identifying that non-punctuation tokens could be utilized toward success by incorporating a loop/list. Which interestingly ended with a print(tokenized[0][11]) as presented united. All of this brings us back to the tokenized vocabulary benefits of establishing a viable dictionary to explore and use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "PUNCT_RE = re.compile(r'[^\\w\\s]+$')\n",
    "\n",
    "def is_punct(string):\n",
    "    \"\"\"Check if STRING is a punctuation marker or a sequence of\n",
    "       punctuation markers.\n",
    "    \"\"\"\n",
    "    return PUNCT_RE.match(string) is not None\n",
    "def preprocess_text(text, language, lowercase=True):\n",
    "    \"\"\"Preprocess a text.\n",
    "\n",
    "    Perform a text preprocessing procedure, which transforms a string\n",
    "    object into a list of word tokens without punctuation markers.\n",
    "\n",
    "    \"\"\"\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(text, language=language)\n",
    "    tokens = [token for token in tokens if not is_punct(token)]\n",
    "    return tokens\n",
    "tokenized = []\n",
    "for text in documents:\n",
    "    tokenized.append(preprocess_text(text, \"english\"))\n",
    "\n",
    "print(tokenized[0][11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Three: Create a document term matrix to enable comparative textual analysis across the full set of documents\n",
    "\n",
    "Bringing in a document term matrix enabled a viable mode of textual analysis that would represent a reason and expansion through the entire listed and introduced data attained to have an opportunity to be compared correctly and openly, as a whole set. Enveloping the format to use the function code of extract_vocabulary() enables a concise way to extract components included in the vocabulary of \"a tokenized corpus given a minimum and a  maximum frequency count\" (Karsdopp, Kestemont, Riddell 87). This coded use helps the amount and size be put through a filter to reduce using finding locational thresholds and allowances of code, which prepares our dictionary of choice to become counted within use or appearance of words presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'_i_\", \"'ad\", \"'ail\", \"'and\", \"'ands\", \"'ardly\", \"'arf\", \"'arf-crowns\", \"'as-is\", \"'at\", \"'ave\", \"'black\", \"'boo\", \"'bus\", \"'but\", \"'certainly\", \"'come\", \"'d\", \"'eard\", \"'eart\", \"'em\", \"'er\", \"'ere\", \"'fan\", \"'here\", \"'if\", \"'im\", \"'ims\", \"'ired\", \"'is\", \"'it\", \"'lark\", \"'life\", \"'ll\", \"'m\", \"'macaroni\", \"'mar\", \"'miss\", \"'movie\", \"'my\", \"'no\", \"'not\", \"'ole\", \"'olidays\", \"'ome\", \"'orse\", \"'ouse\", \"'ow\", \"'phone\", \"'poetry\", \"'re\", \"'s\", \"'that\", \"'the\", \"'there\", \"'they\", \"'thou\", \"'treasure\", \"'uff\", \"'undreds\", \"'varsity\", \"'ve\", \"'well\", \"'what\", \"'why\", \"'yes\", \"'you\", '//gutenberg.org/license', '//pglaf.org', '//pglaf.org/donate', '//pglaf.org/fundraising', '//www.gutenberg.org', '//www.gutenberg.org/fundraising/donate', '//www.pgdp.net', '//www.pglaf.org', '1', '1.', '1.a', '1.b', '1.c', '1.d', '1.e', '1.e.1', '1.e.2', '1.e.3', '1.e.4', '1.e.5', '1.e.6', '1.e.7', '1.e.8', '1.e.9', '1.f', '1.f.1', '1.f.2', '1.f.3', '1.f.4', '1.f.5', '1.f.6', '10', '10.']\n"
     ]
    }
   ],
   "source": [
    "def extract_vocabulary(tokenized_corpus, min_count=1, max_count=float('inf')):\n",
    "    \"\"\"Extract a vocabulary from a tokenized corpus.\n",
    "\n",
    "    Arguments:\n",
    "        tokenized_corpus (list): a tokenized corpus represented, list\n",
    "            of lists of strings.\n",
    "        min_count (int, optional): the minimum occurrence count of a\n",
    "            vocabulary item in the corpus.\n",
    "        max_count (int, optional): the maximum occurrence count of a\n",
    "            vocabulary item in the corpus. Defaults to inf.\n",
    "\n",
    "    Returns:\n",
    "        list: An alphabetically ordered list of unique words in the\n",
    "            corpus, of which the frequencies adhere to the specified\n",
    "            minimum and maximum count.\n",
    "\n",
    "    Examples:\n",
    "        >>> corpus = [['the', 'man', 'love', 'man', 'the'],\n",
    "                      ['the', 'love', 'book', 'wise', 'drama'],\n",
    "                      ['a', 'story', 'book', 'drama']]\n",
    "        >>> extract_vocabulary(corpus, min_count=2)\n",
    "        ['book', 'drama', 'love', 'man', 'the']\n",
    "\n",
    "    \"\"\"\n",
    "    vocabulary = collections.Counter()\n",
    "    for document in tokenized_corpus:\n",
    "        vocabulary.update(document)\n",
    "    vocabulary = {word for word, count in vocabulary.items()\n",
    "                  if count >= min_count and count <= max_count}\n",
    "    return sorted(vocabulary)\n",
    "import collections\n",
    "vocabulary = extract_vocabulary(tokenized, min_count=2)\n",
    "print(vocabulary[0:100])\n",
    "\n",
    "def corpus2dtm(tokenized_corpus, vocabulary):\n",
    "    \"\"\"Transform a tokenized corpus into a document-term matrix.\n",
    "\n",
    "    Arguments:\n",
    "        tokenized_corpus (list): a tokenized corpus as a list of\n",
    "        lists of strings. vocabulary (list): An list of unique words.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lists representing the frequency of each term\n",
    "              in `vocabulary` for each document in the corpus.\n",
    "\n",
    "    Examples:\n",
    "        >>> tokenized_corpus = [['the', 'man', 'man', 'smart'],\n",
    "                                ['a', 'the', 'man' 'love'],\n",
    "                                ['love', 'book', 'journey']]\n",
    "        >>> vocab = ['book', 'journey', 'man', 'love']\n",
    "        >>> corpus2dtm(tokenized_corpus, vocabulary)\n",
    "        [[0, 0, 2, 0], [0, 0, 1, 1], [1, 1, 0, 1]]\n",
    "\n",
    "    \"\"\"\n",
    "    document_term_matrix = []\n",
    "    for document in tokenized_corpus:\n",
    "        document_counts = collections.Counter(document)\n",
    "        row = [document_counts[word] for word in vocabulary]\n",
    "        document_term_matrix.append(row)\n",
    "    return document_term_matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "document_term_matrix = np.array(corpus2dtm(tokenized, vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Four: Chart at least one comparison between the documents, using word frequency to map the text\n",
    "\n",
    "Once Stage Three is found and accomplished, I was able to find frequency within the word account areas of the referenced .txt files established previously within the study. Helping to reveal a visual comparison accompanied by word frequencies to help map the text, Python's use of data structure \"only records distinct elements,\" which brings on the identification of \"all words appearing in it are unique.\" Similarly,  we could construct a vocabulary which excludes the n most\" (Karsdopp, Kestemont, Riddell 87). And since use was enabled within three genres, with the code formation of tr_means = document_term_matrix[genres == 'they'].mean(axis=0), the journey became visual in a sense to enable it to become scaled within the chart (96). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actress: [604  28 861   0 357 102 314]\n",
      "Actor: [ 602  569  838    2 1171  629 1568]\n",
      "Cast: [116 562 177   3 157 454 292]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "actress_id = vocabulary.index('she')\n",
    "actor_id = vocabulary.index('he')\n",
    "cast_id = vocabulary.index('they')\n",
    "\n",
    "actress_counts = document_term_matrix[:, actress_id]\n",
    "actor_counts = document_term_matrix[:, actor_id]\n",
    "cast_counts = document_term_matrix[:, cast_id]\n",
    "print(\"Actress: \" + str(actress_counts))\n",
    "print(\"Actor: \" + str(actor_counts))\n",
    "print(\"Cast: \" + str(cast_counts))\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(len(titles))\n",
    "width = 1/(len(titles))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width, actress_counts, width, label='Actress')\n",
    "rects2 = ax.bar(x, actor_counts, width, label='Actor')\n",
    "rects3 = ax.bar(x + width, cast_counts, width, label='Cast')\n",
    "\n",
    "\n",
    "ax.set_ylabel('Word Count')\n",
    "ax.set_title(' Film Terms Frequency')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(titles)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Five: Calculate the Euclidean distance between the documents, using two key words as the point of comparison\n",
    "\n",
    "Note: Nested four-loop compares and helps to avoid redundancy (loop logic solutions).\n",
    "\n",
    "Euclidean distance's use allows for \"calculating the distance between two documents in a space, ... looks at the exact coordinates of the two documents: it connects them with a straight line, ...and returns the length of that line\" (Karsdopp, Kestemont, Riddell 100). As a result, the comparisons or calculated distances between supplied .txt brought interest and allowed comparisons to take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behind.txt vs boys.txt: 447.21918563496354\n",
      "behind.txt vs fear.txt: 243.75602556654883\n",
      "behind.txt vs living.txt: 610.5481144021329\n",
      "behind.txt vs mystery.txt: 570.4752404793744\n",
      "behind.txt vs togoland.txt: 339.0766874911928\n",
      "behind.txt vs wells.txt: 981.902235459315\n",
      "boys.txt vs fear.txt: 469.6658386555275\n",
      "boys.txt vs living.txt: 796.2223307594431\n",
      "boys.txt vs mystery.txt: 725.5542708853694\n",
      "boys.txt vs togoland.txt: 123.54756169184401\n",
      "boys.txt vs wells.txt: 1034.8434664237873\n",
      "fear.txt vs living.txt: 853.9156867044895\n",
      "fear.txt vs mystery.txt: 333.600059952033\n",
      "fear.txt vs togoland.txt: 347.0014409191985\n",
      "fear.txt vs wells.txt: 739.0027063549903\n",
      "living.txt vs mystery.txt: 1179.1000805699234\n",
      "living.txt vs togoland.txt: 772.3535459878461\n",
      "living.txt vs wells.txt: 1592.4437195706478\n",
      "mystery.txt vs togoland.txt: 618.0396427414669\n",
      "mystery.txt vs wells.txt: 419.32564910818417\n",
      "togoland.txt vs wells.txt: 952.87197461149\n"
     ]
    }
   ],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    \"\"\"Compute the Euclidean distance between two vectors.\n",
    "\n",
    "    Note: ``numpy.linalg.norm(a - b)`` performs the\n",
    "    same calculation using a slightly faster method.\n",
    "\n",
    "    Arguments:\n",
    "        a (numpy.ndarray): a vector of floats or ints.\n",
    "        b (numpy.ndarray): a vector of floats or ints.\n",
    "\n",
    "    Returns:\n",
    "        float: The euclidean distance between vector a and b.\n",
    "\n",
    "    Examples:\n",
    "        >>> import numpy as np\n",
    "        >>> a = np.array([1, 4, 2, 8])\n",
    "        >>> b = np.array([2, 1, 4, 7])\n",
    "        >>> round(euclidean_distance(a, b), 2)\n",
    "        3.87\n",
    "\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "vectors = []\n",
    "for x in range(0, len(titles)):\n",
    "    vectors.append([cast_counts[x],actor_counts[x]])\n",
    "\n",
    "for x in range(0, len(vectors)):\n",
    "    for y in range(x+1, len(vectors)):\n",
    "        comparison = euclidean_distance(np.array(vectors[x]),np.array(vectors[y]))\n",
    "        print(f'{titles[x]} vs {titles[y]}: {comparison}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre - Bonus Stage:\n",
    "\n",
    "Computation of a vector was introduced to enable more value within the Bonus Stage section to help choices of distant modeling, \"def vector_len(v):\" (Karsdopp, Kestemont, Riddell 99)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_len(v):\n",
    "    \"\"\"Compute the length (or norm) of a vector.\"\"\"\n",
    "    return np.sqrt(np.sum(v ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Stage: As a bonus challenge, consider trying one of the other types of distance modeling described in the text.\n",
    "\n",
    "Cosine distance \"is the well-known cosine  distance from geometry\" and differs from Euclidean distance beyond two points importance and bring forth the impact of their identities in arrows/vectors \"that find their offset in the space's origin\" (Karsdopp, Kestemont, Riddell 100). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Distance (#1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor - cast:       0.25\n",
      "actor - actress: 0.24\n",
      "cast - actress: 0.55\n"
     ]
    }
   ],
   "source": [
    "def cosine_distance(a, b):\n",
    "    \"\"\"Compute the cosine distance between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        a (numpy.ndarray): a vector of floats or ints.\n",
    "        b (numpy.ndarray): a vector of floats or ints.\n",
    "\n",
    "    Returns:\n",
    "        float: cosine distance between vector a and b.\n",
    "\n",
    "    Note:\n",
    "        See also scipy.spatial.distance.cdist\n",
    "\n",
    "    Examples:\n",
    "        >>> import numpy as np\n",
    "        >>> a = np.array([1, 4, 2, 8])\n",
    "        >>> b = np.array([2, 1, 4, 7])\n",
    "        >>> round(cosine_distance(a, b), 2)\n",
    "        0.09\n",
    "\n",
    "    \"\"\"\n",
    "    return 1 - np.dot(a, b) / (vector_len(a) * vector_len(b))\n",
    "\n",
    "\n",
    "# %%\n",
    "tc = cosine_distance(actor_counts, cast_counts)\n",
    "print(f'actor - cast:       {tc:.2f}')\n",
    "\n",
    "ttc = cosine_distance(actor_counts, actress_counts)\n",
    "print(f'actor - actress: {ttc:.2f}')\n",
    "\n",
    "ctc = cosine_distance(cast_counts, actress_counts)\n",
    "print(f'cast - actress: {ctc:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Distance (#2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor - cast:       0.25\n",
      "actor - actress: 0.24\n",
      "cast - actress: 0.55\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def cosine_distance(a, b):\n",
    "    \"\"\"Compute the cosine distance between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        a (numpy.ndarray): a vector of floats or ints.\n",
    "        b (numpy.ndarray): a vector of floats or ints.\n",
    "\n",
    "    Returns:\n",
    "        float: cosine distance between vector a and b.\n",
    "\n",
    "    Note:\n",
    "        See also scipy.spatial.distance.cdist\n",
    "\n",
    "    Examples:\n",
    "        >>> import numpy as np\n",
    "        >>> a = np.array([1, 4, 2, 8])\n",
    "        >>> b = np.array([2, 1, 4, 7])\n",
    "        >>> round(cosine_distance(a, b), 2)\n",
    "        0.09\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1 - np.dot(a, b) / (vector_len(a) * vector_len(b))\n",
    "\n",
    "\n",
    "# %%\n",
    "tc = cosine_distance(actor_counts, cast_counts)\n",
    "print(f'actor - cast:       {tc:.2f}')\n",
    "\n",
    "ttc = cosine_distance(actor_counts, actress_counts)\n",
    "print(f'actor - actress: {ttc:.2f}')\n",
    "\n",
    "ctc = cosine_distance(cast_counts, actress_counts)\n",
    "print(f'cast - actress: {ctc:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City Block Distance\n",
    "\n",
    "City block distance (aka Manhattan distance/L1 distance) \"is a metric which computes the distance between two  points in space as the sum of the absolute differences of their coordinates in  space\" (Karsdopp, Kestemont, Riddell 102). In this code example, I \"plotted the individual paths between the data  points\" (104). and even though it is not used that frequently in text analysis, it \"is a well-known distance function whose \"inner workings\" are not too hard to understand\" (105). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_block_distance(a, b):\n",
    "    \"\"\"Compute the city block distance between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        a (numpy.ndarray): a vector of floats or ints.\n",
    "        b (numpy.ndarray): a vector of floats or ints.\n",
    "\n",
    "    Returns:\n",
    "        {int, float}: The city block distance between vector a and b.\n",
    "\n",
    "    Examples:\n",
    "        >>> import numpy as np\n",
    "        >>> a = np.array([1, 4, 2, 8])\n",
    "        >>> b = np.array([2, 1, 4, 7])\n",
    "        >>> city_block_distance(a, b)\n",
    "        7\n",
    "\n",
    "    \"\"\"\n",
    "    return np.abs(a - b).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cast - actor: 1833.08\n",
      "cast - actress: 1074.97\n",
      "actor - actress: 1675.13\n"
     ]
    }
   ],
   "source": [
    "tc = euclidean_distance(actor_counts, cast_counts)  \n",
    "print(f'cast - actor: {tc:.2f}')  \n",
    "\n",
    "ttc = euclidean_distance(they_counts, girl_counts)  \n",
    "print(f'cast - actress: {ttc:.2f}') \n",
    "\n",
    "ctc = euclidean_distance(boy_counts, girl_counts) \n",
    "print(f'actor - actress: {ctc:.2f}') \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "598d64e08a06bfba065a99698ec5bbec753236817de80e4f3bcf221574aa140c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
